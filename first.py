import librosa
import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# Load model and processor locally
device = "cuda" if torch.cuda.is_available() else "cpu"

try:
    print("ğŸ”„ Loading model from 'whisper_model'...")
    model_name = "openai/whisper-medium"
    processor = WhisperProcessor.from_pretrained(model_name)
    model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)
    print("âœ… Model loaded successfully!")
except Exception as e:
    print("âŒ Error loading model:", e)
    exit(1)  # Exit script if model fails to load


# Load model and processor locally
device = "cuda" if torch.cuda.is_available() else "cpu"


def transcribe(audio_path):
    print(f"ğŸ“¢ Processing file: {audio_path}")

    # Load audio
    audio, sr = librosa.load(audio_path, sr=16000)
    print(f"âœ… Loaded {len(audio)} samples at {sr} Hz")

    if len(audio) == 0:
        print("âŒ Audio is empty!")
        return "Error: Empty Audio"

    # Tokenize input
    input_features = processor(audio, sampling_rate=16000, return_tensors="pt").input_features.to(device)
    print(f"ğŸ”„ Input feature shape: {input_features.shape}")

    if input_features.shape[1] == 0:
        print("âŒ Input features are empty! Check audio file.")
        return "Error: Empty Input Features"

    # Generate transcription
    try:
        print("ğŸ™ï¸ Generating transcription...")
        inputs = processor(audio, sampling_rate=16000, return_tensors="pt", return_attention_mask=True)
        input_features = inputs.input_features.to(device)
        attention_mask = inputs.attention_mask.to(device)

        predicted_ids = model.generate(input_features, attention_mask=attention_mask)
        # print(f"ğŸ”¤ Predicted token IDs Shape: {predicted_ids.shape}")
        # print(f"ğŸ”¤ Predicted token IDs: {predicted_ids}")
    except Exception as e:
        print("âŒ Error during generation:", e)
        return "Error in Model Generation"

    # Check if output is empty
    if predicted_ids.numel() == 0:
        print("âŒ Model returned empty output!")
        return "No transcription"

    # Decode transcription
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription


# Test Function
text = transcribe("rec2.wav")
print("ğŸ“ Transcription:", text)
